import pandas as pd
import pytest
import plotly.graph_objs
import numpy as np
import os
from unittest.mock import patch, mock_open, MagicMock
from PIL import Image # For mocking Image.open related errors
import datetime # For predictable date string in file save tests


from nlplot import NLPlot, get_colorpalette, generate_freq_df, DEFAULT_FONT_PATH as NLPLOT_DEFAULT_FONT_PATH # Import for mocking

# Helper to create a dummy font file if it doesn't exist.
def ensure_font_file_exists(font_path_to_check = NLPLOT_DEFAULT_FONT_PATH):
    font_dir = os.path.dirname(font_path_to_check)
    if not os.path.exists(font_path_to_check):
        if font_dir: # Ensure directory exists only if font_dir is not empty (e.g. for relative paths)
            os.makedirs(font_dir, exist_ok=True)
        try:
            with open(font_path_to_check, 'w') as f:
                f.write("")
        except Exception as e:
            print(f"Could not create placeholder font file at {font_path_to_check}: {e}")
    return font_path_to_check

# Ensure the default font path used by nlplot exists for tests that rely on it.
# This TTF_FONT_PATH will be the one nlplot.nlplot.DEFAULT_FONT_PATH points to.
TTF_FONT_PATH = ensure_font_file_exists()


@pytest.fixture
def prepare_data():
    target_col = "text"
    texts = ["Think rich look poor",
             "When you come to a roadblock, take a detour",
             "When it is dark enough, you can see the stars",
             "Never let your memories be greater than your dreams",
             "Victory is sweetest when youâ€™ve known defeat"]
    return pd.DataFrame({target_col: texts})

@pytest.fixture
def empty_data():
    return pd.DataFrame({"text": []})

@pytest.fixture
def data_with_empty_text():
    return pd.DataFrame({"text": ["", "  ", "Another sentence"]})


@pytest.fixture
def prepare_instance(prepare_data, tmp_path):
    output_dir = tmp_path / "test_outputs"
    os.makedirs(output_dir, exist_ok=True)
    # For these tests, we want NLPlot to try and use its actual default font logic first.
    # So, don't pass font_path to constructor unless testing that specific override.
    return NLPlot(prepare_data.copy(), target_col="text", output_file_path=str(output_dir))

@pytest.fixture
def prepare_instance_custom_font(prepare_data, tmp_path):
    output_dir = tmp_path / "test_outputs_custom_font"
    os.makedirs(output_dir, exist_ok=True)
    custom_font = tmp_path / "custom_font.ttf" # Create a dummy custom font
    ensure_font_file_exists(str(custom_font))
    return NLPlot(prepare_data.copy(), target_col="text", output_file_path=str(output_dir), font_path=str(custom_font))


@pytest.fixture
def prepare_instance_empty_df(empty_data, tmp_path):
    output_dir = tmp_path / "test_outputs_empty"
    os.makedirs(output_dir, exist_ok=True)
    return NLPlot(empty_data.copy(), target_col="text", output_file_path=str(output_dir))


@pytest.fixture
def series_data_for_freq_df(prepare_data):
    return prepare_data["text"].copy()


# --- Test get_colorpalette ---
@pytest.mark.parametrize(
    "color_palette, n_legends, expected_len_check",
    [("hls", 1, lambda l: l == 1), ("hls", 10, lambda l: l == 10)]
)
def test_get_colorpalette_valid(color_palette, n_legends, expected_len_check):
    rgbs = get_colorpalette(color_palette, n_legends)
    assert isinstance(rgbs, list)
    assert expected_len_check(len(rgbs))
    if n_legends > 0:
        assert all(isinstance(rgb_str, str) and rgb_str.startswith("rgb(") for rgb_str in rgbs)

@pytest.mark.parametrize("invalid_n_colors", [0, -1, 0.5, "abc"])
def test_get_colorpalette_invalid_n_colors(invalid_n_colors):
    with pytest.raises(ValueError, match="n_colors must be a positive integer"):
        get_colorpalette("hls", invalid_n_colors)


# --- Test generate_freq_df ---
@pytest.mark.parametrize(
    "n_gram, top_n, expected_rows_check, check_content_type",
    [
        (1, 1, lambda r: r <= 1, True), (1, 50, lambda r: r <= 50, True),
        (2, 3, lambda r: r <= 3, True), (1, 0, lambda r: r == 0, False),
    ]
)
def test_generate_freq_df_valid(series_data_for_freq_df, n_gram, top_n, expected_rows_check, check_content_type):
    word_frequency = generate_freq_df(series_data_for_freq_df, n_gram=n_gram, top_n=top_n, stopwords=[], verbose=False)
    expect_columns = ["word", "word_count"]
    assert isinstance(word_frequency, pd.DataFrame)
    assert word_frequency.ndim == 2
    assert expected_rows_check(len(word_frequency))
    assert list(word_frequency.columns) == expect_columns
    if check_content_type and top_n > 0 and not word_frequency.empty:
        assert isinstance(word_frequency["word"].iloc[0], str)
        assert isinstance(word_frequency["word_count"].iloc[0], (int, np.integer))

@pytest.mark.parametrize("invalid_n_gram", [0, -1, 0.5, "abc"])
def test_generate_freq_df_invalid_n_gram(series_data_for_freq_df, invalid_n_gram):
    with pytest.raises(ValueError, match="n_gram must be a positive integer"):
        generate_freq_df(series_data_for_freq_df, n_gram=invalid_n_gram, top_n=10)

@pytest.mark.parametrize("invalid_top_n", [-1, 0.5, "abc"])
def test_generate_freq_df_invalid_top_n(series_data_for_freq_df, invalid_top_n):
    with pytest.raises(ValueError, match="top_n must be a non-negative integer"):
        generate_freq_df(series_data_for_freq_df, n_gram=1, top_n=invalid_top_n)

def test_generate_freq_df_empty_input_series():
    empty_series = pd.Series([], dtype=str)
    df_empty = generate_freq_df(empty_series, n_gram=1, top_n=10)
    assert df_empty.empty
    assert list(df_empty.columns) == ["word", "word_count"]

def test_generate_freq_df_with_stopwords(series_data_for_freq_df):
    stopwords = ["you", "to"]
    df_with_stopwords = generate_freq_df(series_data_for_freq_df, n_gram=1, top_n=10, stopwords=stopwords)
    if not df_with_stopwords.empty:
        assert "you" not in df_with_stopwords["word"].tolist()
        assert "to" not in df_with_stopwords["word"].tolist()


# --- Test NLPlot class ---
def test_nlplot_init(prepare_data, tmp_path):
    output_path = tmp_path / "init_test_out"
    npt = NLPlot(prepare_data.copy(), target_col="text", output_file_path=str(output_path))
    assert isinstance(npt.df, pd.DataFrame)
    if not npt.df.empty:
        assert isinstance(npt.df["text"].iloc[0], list)
    assert npt.output_file_path == str(output_path)
    assert npt.font_path == TTF_FONT_PATH # Should fallback to default if not specified

@patch("builtins.print")
def test_nlplot_init_custom_font_found(mock_print, prepare_data, tmp_path):
    custom_font_file = tmp_path / "my_custom.ttf"
    ensure_font_file_exists(str(custom_font_file)) # Create dummy custom font
    npt = NLPlot(prepare_data.copy(), target_col="text", font_path=str(custom_font_file))
    assert npt.font_path == str(custom_font_file)
    mock_print.assert_not_called() # No warning if font is found

@patch("builtins.print")
def test_nlplot_init_custom_font_not_found(mock_print, prepare_data):
    non_existent_font = "non_existent_custom_font.ttf"
    npt = NLPlot(prepare_data.copy(), target_col="text", font_path=non_existent_font)
    assert npt.font_path == TTF_FONT_PATH # Should fallback to default
    mock_print.assert_any_call(f"Warning: Specified font_path '{non_existent_font}' not found. Falling back to default: {TTF_FONT_PATH}")

@patch("builtins.print")
@patch("nlplot.nlplot.DEFAULT_FONT_PATH", "truly_missing_default.ttf") # Mock the default path to non-existent
@patch("os.path.exists", side_effect=lambda p: False if p == "truly_missing_default.ttf" else os.path.exists(p)) # only default is missing
def test_nlplot_init_default_font_missing_warning(mock_os_exists, mock_print, prepare_data):
    npt = NLPlot(prepare_data.copy(), target_col="text")
    assert npt.font_path == "truly_missing_default.ttf" # It will still be set to this path
    mock_print.assert_any_call(f"Warning: The determined font path 'truly_missing_default.ttf' does not exist. WordCloud may fail if a valid font is not provided at runtime.")


@patch("builtins.print")
def test_nlplot_init_with_default_stopwords_file(mock_print, tmp_path, prepare_data):
    stopwords_content = "stopword1\nstopword2\n"
    p = tmp_path / "stopwords.txt"
    p.write_text(stopwords_content)
    df = pd.DataFrame({"text": ["stopword1 and testword1"]})
    npt = NLPlot(df, target_col="text", default_stopwords_file_path=str(p))
    assert "stopword1" in npt.default_stopwords
    assert "stopword2" in npt.default_stopwords
    stopwords_calc = npt.get_stopword(top_n=0, min_freq=0)
    assert "stopword1" in stopwords_calc

@patch("builtins.print")
@patch("builtins.open", side_effect=PermissionError("Test permission error for reading stopwords"))
def test_nlplot_init_stopwords_permission_error(mock_open, mock_print, prepare_data, tmp_path):
    dummy_sw_path = str(tmp_path / "dummy_stopwords.txt")
    with patch("os.path.exists", return_value=True):
        npt = NLPlot(prepare_data.copy(), target_col="text", default_stopwords_file_path=dummy_sw_path)
    assert npt.default_stopwords == []
    printed_warnings = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Warning: Permission denied to read stopwords file {dummy_sw_path}" in printed_warnings

@patch("builtins.print")
@patch("builtins.open", side_effect=IOError("Test IO error for reading stopwords"))
def test_nlplot_init_stopwords_io_error(mock_open, mock_print, prepare_data, tmp_path):
    dummy_sw_path = str(tmp_path / "dummy_stopwords.txt")
    with patch("os.path.exists", return_value=True):
        npt = NLPlot(prepare_data.copy(), target_col="text", default_stopwords_file_path=dummy_sw_path)
    assert npt.default_stopwords == []
    printed_warnings = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Warning: Could not read stopwords file {dummy_sw_path} due to an IO error" in printed_warnings


def test_nlplot_init_with_malformed_text_column(prepare_data):
    df_malformed = prepare_data.copy()
    df_malformed.loc[0, "text"] = 12345
    npt = NLPlot(df_malformed, target_col="text")
    assert isinstance(npt.df["text"].iloc[0], list)
    assert npt.df["text"].iloc[0] == ["12345"]

@pytest.mark.parametrize("top_n, min_freq", [(1,0), (0,1), (0,0)])
def test_nlplot_get_stopword(prepare_instance, top_n, min_freq):
    npt = prepare_instance
    stopwords = npt.get_stopword(top_n=top_n, min_freq=min_freq)
    assert isinstance(stopwords, list)


@pytest.mark.parametrize("invalid_input", [-1, "a"])
def test_nlplot_get_stopword_invalid_params(prepare_instance, invalid_input):
    npt = prepare_instance
    with pytest.raises(ValueError):
        npt.get_stopword(top_n=invalid_input, min_freq=0)
    with pytest.raises(ValueError):
        npt.get_stopword(top_n=0, min_freq=invalid_input)


# --- Test Plotting Methods ---
@patch("plotly.offline.plot")
def test_nlplot_bar_ngram(mock_plotly_plot, prepare_instance):
    npt = prepare_instance
    fig = npt.bar_ngram(title='uni-gram', ngram=1, top_n=5, save=True)
    assert isinstance(fig, plotly.graph_objs.Figure)
    if not npt.ngram_df.empty :
         assert "word" in npt.ngram_df.columns
    mock_plotly_plot.assert_called_once()

@patch("builtins.print")
@patch("plotly.offline.plot", side_effect=PermissionError("Test permission error for plot"))
@patch("os.makedirs")
def test_save_plot_permission_error(mock_makedirs, mock_plotly_plot_err, mock_print, prepare_instance):
    npt = prepare_instance
    fig = plotly.graph_objs.Figure()
    with patch('nlplot.nlplot.datetime') as mock_datetime:
        mock_datetime.datetime.now.return_value = datetime.datetime(2023, 1, 1, 12, 0, 0)
        npt.save_plot(fig, "test_plot_perm_error")
    printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert "Error: Permission denied to write to" in printed_output
    assert "test_plot_perm_error.html. Please check directory permissions." in printed_output


@patch("plotly.offline.plot")
def test_nlplot_treemap(mock_plotly_plot, prepare_instance):
    npt = prepare_instance
    fig = npt.treemap(title='Tree Map', ngram=1, top_n=5, save=True)
    assert isinstance(fig, plotly.graph_objs.Figure)
    if not npt.treemap_df.empty:
        assert "word" in npt.treemap_df.columns
    mock_plotly_plot.assert_called_once()

@patch("plotly.offline.plot")
def test_nlplot_word_distribution(mock_plotly_plot, prepare_instance):
    npt = prepare_instance
    fig = npt.word_distribution(title='Word Dist', save=True)
    assert isinstance(fig, plotly.graph_objs.Figure)
    assert (npt.target_col + '_length') in npt.df.columns
    mock_plotly_plot.assert_called_once()


@patch("IPython.display.display")
@patch("PIL.Image.Image.save")
@patch("nlplot.nlplot.DEFAULT_FONT_PATH", TTF_FONT_PATH) # Ensure test uses a known valid default path
def test_nlplot_wordcloud_default_font(mock_pil_save, mock_ipython_display, prepare_instance):
    npt = prepare_instance # This instance uses the default font path (TTF_FONT_PATH)
    try:
        npt.wordcloud(save=True)
    except Exception as e:
        pytest.fail(f"wordcloud with default font raised an exception: {e}")
    mock_ipython_display.assert_called_once()
    if npt.df[npt.target_col].apply(lambda x: bool(x)).any():
         assert mock_pil_save.call_count >= 1 # Stream save
         if True: # save=True
            assert mock_pil_save.call_count == 2 # Stream and file save


@patch("builtins.print")
@patch("IPython.display.display")
@patch("PIL.Image.Image.save")
@patch("nlplot.nlplot.DEFAULT_FONT_PATH", TTF_FONT_PATH) # Valid default for fallback
def test_nlplot_wordcloud_invalid_font_fallback_to_default(mock_pil_save, mock_display, mock_print, prepare_instance, tmp_path):
    npt = prepare_instance

    # Mock WordCloud constructor: first call (invalid font) raises OSError, second call (default font) succeeds
    mock_wc_successful_instance = MagicMock()
    mock_wc_successful_instance.generate = MagicMock()
    mock_wc_successful_instance.to_array = MagicMock(return_value=np.array([[[0,0,0]]], dtype=np.uint8))

    # This mock will be used for the nlplot.nlplot.WordCloud class
    mock_wordcloud_class = MagicMock()
    mock_wordcloud_class.side_effect = [
        OSError("Simulated font error with custom_invalid_font.ttf"), # First attempt
        mock_wc_successful_instance # Second attempt (fallback to default)
    ]

    invalid_font_path = "custom_invalid_font.ttf" # Does not exist / is invalid

    with patch("nlplot.nlplot.WordCloud", mock_wordcloud_class):
        with patch("os.path.exists") as mock_os_exists:
            # Setup os.path.exists:
            # 1. custom_invalid_font.ttf -> True (pretend it exists but is invalid)
            # 2. TTF_FONT_PATH (default) -> True (it should exist as per ensure_font_file_exists)
            def side_effect_os_exists(path):
                if path == invalid_font_path: return True
                if path == TTF_FONT_PATH: return True
                return os.path.exists(path) # original for other paths
            mock_os_exists.side_effect = side_effect_os_exists

            npt.wordcloud(font_path=invalid_font_path)

    printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Warning: Error processing font at '{invalid_font_path}'" in printed_output
    assert f"Attempting to fallback to default font: {TTF_FONT_PATH}" in printed_output
    mock_wc_successful_instance.generate.assert_called_once() # Fallback was successful
    mock_display.assert_called_once()


@patch("builtins.print")
@patch("IPython.display.display")
@patch("nlplot.nlplot.WordCloud") # Mock WordCloud class entirely for this
@patch("nlplot.nlplot.DEFAULT_FONT_PATH", "truly_missing_default.ttf")
@patch("os.path.exists")
def test_nlplot_wordcloud_custom_font_fails_default_font_missing(mock_os_exists, mock_WordCloud_class, mock_display, mock_print, prepare_instance):
    npt = prepare_instance

    invalid_custom_font = "another_invalid_font.ttf"
    # os.path.exists: 1. invalid_custom_font (True), 2. truly_missing_default.ttf (False)
    mock_os_exists.side_effect = lambda p: True if p == invalid_custom_font else (False if p == "truly_missing_default.ttf" else os.path.exists(p))

    # First WordCloud attempt (with invalid_custom_font) should raise OSError
    mock_WordCloud_class.side_effect = OSError("Error with invalid_custom_font")

    npt.wordcloud(font_path=invalid_custom_font)

    printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Warning: Error processing font at '{invalid_custom_font}'" in printed_output
    assert f"Attempting to fallback to default font: truly_missing_default.ttf" in printed_output # Tries to fallback
    assert "Error: Default font not found at 'truly_missing_default.ttf' and custom font failed." in printed_output
    mock_display.assert_not_called() # Should not display anything


@patch("builtins.print")
@patch("PIL.Image.open", side_effect=PermissionError("Test permission error for mask"))
@patch("IPython.display.display")
def test_nlplot_wordcloud_mask_permission_error(mock_display, mock_pil_open_err, mock_print, prepare_instance):
    npt = prepare_instance
    dummy_mask_path = "dummy_mask.png"
    with patch("os.path.exists", return_value=True):
        npt.wordcloud(mask_file=dummy_mask_path)
    printed_warnings = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Warning: Permission denied to read mask file {dummy_mask_path}" in printed_warnings


@patch("plotly.offline.iplot")
@patch("plotly.offline.plot")
def test_nlplot_co_network(mock_plotly_plot, mock_iplot, prepare_instance):
    npt = prepare_instance
    npt.build_graph(min_edge_frequency=0)
    assert hasattr(npt, 'G')
    if not npt.node_df.empty:
        npt.co_network(title='Co-occurrence', save=True)
        mock_iplot.assert_called_once()
        mock_plotly_plot.assert_called_once()
    else:
        with patch('builtins.print') as mock_print:
            npt.co_network(title='Co-occurrence')
            printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
            assert "Graph not built or empty" in printed_output or "Node DataFrame not available or empty" in printed_output


@patch("plotly.offline.plot")
def test_nlplot_sunburst(mock_plotly_plot, prepare_instance):
    npt = prepare_instance
    npt.build_graph(min_edge_frequency=0)
    if not npt.node_df.empty:
        fig = npt.sunburst(title='Sunburst', save=True, colorscale=True)
        assert isinstance(fig, plotly.graph_objs.Figure)
        mock_plotly_plot.assert_called_once()
    else:
        with patch('builtins.print') as mock_print:
            fig = npt.sunburst(title='Sunburst')
            printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
            assert "Node DataFrame not available or empty" in printed_output
            assert isinstance(fig, plotly.graph_objs.Figure)


@patch("builtins.print")
@patch("pandas.DataFrame.to_csv", side_effect=PermissionError("Test permission error for CSV save"))
@patch("os.makedirs")
def test_save_tables_permission_error(mock_makedirs, mock_to_csv_err, mock_print, prepare_instance):
    npt = prepare_instance
    npt.node_df = pd.DataFrame({'id': ['node1']})
    npt.edge_df = pd.DataFrame({'source': ['node1']})
    npt.save_tables(prefix="test_save_perm_error")
    printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert f"Error: Permission denied to write tables in {npt.output_file_path}" in printed_output


# --- Tests for empty or problematic data ---
@patch("plotly.offline.plot")
def test_nlplot_bar_ngram_empty_df(mock_plotly_plot, prepare_instance_empty_df):
    npt = prepare_instance_empty_df
    with patch('builtins.print') as mock_print:
        fig = npt.bar_ngram(title='uni-gram', ngram=1, top_n=5)
        assert isinstance(fig, plotly.graph_objs.Figure)
        printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
        assert "No data to plot for bar_ngram" in printed_output


def test_nlplot_build_graph_empty_df(prepare_instance_empty_df):
    npt = prepare_instance_empty_df
    with patch('builtins.print') as mock_print:
        npt.build_graph()
        printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
        assert "No nodes found" in printed_output
        assert npt.node_df.empty


def test_nlplot_init_data_with_empty_text(data_with_empty_text, tmp_path):
    output_dir = tmp_path / "empty_text_outputs"
    npt = NLPlot(data_with_empty_text, target_col="text", output_file_path=str(output_dir))
    assert isinstance(npt.df["text"].iloc[0], list)
    assert npt.df["text"].iloc[0] == []
    assert isinstance(npt.df["text"].iloc[1], list)
    assert npt.df["text"].iloc[1] == []
    assert npt.df["text"].iloc[2] == ["Another", "sentence"]


# --- Tests for special text inputs ---
@pytest.mark.parametrize("special_text, expected_token_presence", [
    ("word\x00withnull", "wordwithnull"), # How null bytes are handled by str.split can vary.
    ("emojiğŸ˜€text", "emojiğŸ˜€text"),
    ("very"+"long"*1000+"word", "very"+"long"*1000+"word"),
    ("  leading space", "leading"),
    ("trailing space  ", "space"),
    ("multiple   spaces", "multiple"),
    ("\x01\x02\x03", "\x01\x02\x03"), # Control chars as a single token if not split by space
])
def test_generate_freq_df_with_special_texts(special_text, expected_token_presence):
    test_series = pd.Series([special_text, "normal text"])
    # verbose=False to avoid tqdm in tests
    df_freq = generate_freq_df(test_series, n_gram=1, top_n=5, stopwords=[], verbose=False)

    # Primarily assert that it doesn't crash and produces some output if special_text isn't empty
    if special_text.strip(): # If the special text isn't just whitespace
        assert not df_freq.empty, f"Freq df is empty for special text: '{special_text[:20]}...'"
        # Check if the expected token or a processed version of it appears.
        # This is a loose check as tokenization of special chars can be complex.
        # For null bytes, str.split() might treat them as part of a word or remove them.
        # Python's str.split() on space doesn't typically remove null bytes within words.
        found = any(expected_token_presence in word for word in df_freq["word"].tolist())
        # assert found, f"Expected token part '{expected_token_presence}' not found in words from '{special_text[:20]}...'. Words: {df_freq['word'].tolist()}"
        # For now, mainly concerned it doesn't crash. Actual token content might need more specific handling in generate_ngrams if issues arise.
    else: # If special_text is only whitespace, df_freq might be empty or contain only "normal", "text"
        pass


@patch("builtins.print") # To capture warnings/errors
@patch("plotly.offline.plot")
@patch("plotly.offline.iplot")
@patch("IPython.display.display")
@patch("PIL.Image.Image.save")
def test_nlplot_methods_with_special_texts(mock_pil_save, mock_ipython_display, mock_iplot, mock_plotly_plot, mock_print, prepare_instance):
    npt = prepare_instance

    # Sample texts with various special conditions
    # Note: WordCloud is particularly sensitive to characters not in the font.
    # Null bytes (\x00) are problematic for many C libraries underlying Python packages.
    special_texts_as_lists = [
        ["word\x01with\x02control\x03chars"], # Some control chars (SOH, STX, ETX)
        ["emojiğŸ˜€ğŸ˜ƒğŸ˜„ğŸ˜text"],                # Emojis
        ["very"+"long"*200+"word", "another"+"short"*200+"token"], # Long words (reduced length for performance)
        ["  "],                             # Whitespace only (becomes empty list of words)
        [""],                               # Empty string (becomes empty list of words)
        ["\x00\x00\x00"],                   # Null bytes only
    ]
    # Create a new DataFrame for this test to avoid prepare_instance's default data
    df_special = pd.DataFrame({npt.target_col: [" ".join(text_list) for text_list in special_texts_as_lists]})
    npt_special = NLPlot(df_special, target_col=npt.target_col, output_file_path=npt.output_file_path, font_path=npt.font_path)

    # Test each relevant method
    npt_special.bar_ngram()
    npt_special.treemap()
    npt_special.word_distribution()

    try:
        npt_special.wordcloud()
    except Exception as e:
        # Allow certain errors from WordCloud with problematic chars if they are not handled by nlplot's sanitization (which is minimal)
        print(f"Wordcloud with special text info: {e}")
        # Depending on policy, might fail: pytest.fail(f"Wordcloud failed with special text: {e}")

    npt_special.build_graph(min_edge_frequency=0)
    if hasattr(npt_special, 'node_df') and not npt_special.node_df.empty:
        npt_special.co_network()
        npt_special.sunburst()
    else:
        print("Skipping network/sunburst due to empty node_df with special text.")

    # Assert that no unexpected exceptions were raised by nlplot itself (mocked functions don't count)
    # mock_print might contain warnings, which is acceptable.


@patch("builtins.print")
@patch("plotly.offline.plot") # Mock plotting to prevent actual plot generation
@patch("plotly.offline.iplot")
@patch("IPython.display.display")
@patch("PIL.Image.Image.save")
def test_nlplot_all_stopwords(mock_save, mock_display, mock_iplot, mock_plot, mock_print, prepare_instance):
    npt = prepare_instance

    # Create a list of all unique words from the sample data in the instance
    all_words_in_sample = list(set(word for doc_word_list in npt.df[npt.target_col] for word in doc_word_list if word.strip()))

    if not all_words_in_sample: # Should not happen with prepare_instance's data
        pytest.skip("Sample data resulted in no words, cannot test all_stopwords scenario.")

    # Use these words as stopwords for the plotting methods
    # We are not mocking npt.get_stopword here, but passing the list directly.

    fig_bar = npt.bar_ngram(stopwords=all_words_in_sample)
    assert isinstance(fig_bar, plotly.graph_objs.Figure) # Expect empty figure

    fig_tree = npt.treemap(stopwords=all_words_in_sample)
    assert isinstance(fig_tree, plotly.graph_objs.Figure)

    npt.wordcloud(stopwords=all_words_in_sample) # Should print warning and return

    npt.build_graph(stopwords=all_words_in_sample, min_edge_frequency=0)
    # build_graph should result in empty node_df and edge_df
    assert npt.node_df.empty if hasattr(npt, 'node_df') else True
    assert npt.edge_df.empty if hasattr(npt, 'edge_df') else True

    npt.co_network() # Should warn and not plot
    npt.sunburst()   # Should warn and not plot

    # Verify that appropriate warnings were printed
    printed_output = "".join(call.args[0] for call in mock_print.call_args_list if call.args)
    assert "No data to plot for bar_ngram" in printed_output
    assert "No data to plot for treemap" in printed_output
    assert ("WordCloud could not be generated" in printed_output or "Text corpus is empty" in printed_output or "All words might have been filtered out" in printed_output)
    assert "No nodes found after processing for build_graph" in printed_output
    # If build_graph prints "No nodes found", subsequent co_network/sunburst calls will print "Graph not built or empty" or "Node DataFrame not available"
    assert ("Graph not built or empty" in printed_output or "Node DataFrame not available or empty" in printed_output)


# --- Tests for Japanese text processing (TDD for new feature) ---
# Import JanomeToken for type checking if JANOME_AVAILABLE
try:
    from janome.tokenizer import Token as JanomeTokenInTest
    JANOME_INSTALLED_FOR_TEST = True
except ImportError:
    JANOME_INSTALLED_FOR_TEST = False
    class JanomeTokenInTest: pass # Dummy for type hints if not installed

def test_tokenize_japanese_text(prepare_instance):
    """
    (Green/Refactor Phase for TDD Cycle 1)
    Tests that the _tokenize_japanese_text method tokenizes Japanese text correctly.
    """
    npt = prepare_instance
    text_to_tokenize = "çŒ«ãŒçª“ã‹ã‚‰é¡”ã‚’å‡ºã—ãŸã€‚"

    if not nlplot.nlplot.JANOME_AVAILABLE: # Check the flag from the nlplot module itself
        pytest.skip("Janome not installed or not available in nlplot module, skipping detailed tokenization test.")

    tokens = npt._tokenize_japanese_text(text_to_tokenize)

    assert tokens is not None, "Tokenization returned None, expected a list."
    assert isinstance(tokens, list), f"Expected a list of tokens, got {type(tokens)}"

    if npt._janome_tokenizer is None :
         pytest.skip("Janome tokenizer was not initialized properly in NLPlot instance.")

    assert len(tokens) > 0, "Tokenization returned an empty list for non-empty input."

    expected_surfaces = ["çŒ«", "ãŒ", "çª“", "ã‹ã‚‰", "é¡”", "ã‚’", "å‡ºã—", "ãŸ", "ã€‚"]
    token_surfaces = [token.surface for token in tokens]
    assert token_surfaces == expected_surfaces, f"Token surfaces differ. Expected {expected_surfaces}, got {token_surfaces}"

    expected_pos_tuples = [
        ("çŒ«", "åè©,ä¸€èˆ¬"), ("ãŒ", "åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬"), ("çª“", "åè©,ä¸€èˆ¬"), ("ã‹ã‚‰", "åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬"),
        ("é¡”", "åè©,ä¸€èˆ¬"), ("ã‚’", "åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬"), ("å‡ºã—", "å‹•è©,è‡ªç«‹"),
        ("ãŸ", "åŠ©å‹•è©"), ("ã€‚", "è¨˜å·,å¥ç‚¹"),
    ]

    assert len(tokens) == len(expected_pos_tuples), "Number of tokens does not match expected."
    for i, token in enumerate(tokens):
        if JANOME_INSTALLED_FOR_TEST: # Only assert type if Janome is installed in test env
             assert isinstance(token, JanomeTokenInTest), f"Token {i} is not a JanomeTokenInTest, got {type(token)}"
        assert token.surface == expected_pos_tuples[i][0]
        assert token.part_of_speech.startswith(expected_pos_tuples[i][1]), \
               f"Token {i} ('{token.surface}') POS mismatch. Expected prefix '{expected_pos_tuples[i][1]}', got '{token.part_of_speech}'"

    # Test edge cases for _tokenize_japanese_text
    assert npt._tokenize_japanese_text("") == [], "Empty string should return empty list."
    assert npt._tokenize_japanese_text("   ") == [], "Whitespace-only string should return empty list."
    # Test with non-string input (current implementation returns empty list)
    assert npt._tokenize_japanese_text(None) == [], "None input should return empty list." # type: ignore
    assert npt._tokenize_japanese_text(123) == [], "Integer input should return empty list."   # type: ignore


@patch("nlplot.nlplot.JANOME_AVAILABLE", False)
@patch("builtins.print")
def test_tokenize_japanese_text_janome_not_available(mock_print, prepare_instance):
    npt = prepare_instance
    text = "ãƒ†ã‚¹ãƒˆ"
    tokens = npt._tokenize_japanese_text(text)
    assert tokens == []
    mock_print.assert_any_call("Warning: Janome is not installed. Japanese tokenization is not available. Please install Janome (e.g., pip install janome).")


@patch.object(nlplot.nlplot.JanomeTokenizer, 'tokenize', side_effect=Exception("Simulated Janome Error"))
@patch("builtins.print")
def test_tokenize_japanese_text_janome_tokenization_error(mock_print, mock_tokenize_method, prepare_instance):
    # This test assumes JANOME_AVAILABLE is True, but the tokenize call itself fails
    if not nlplot.nlplot.JANOME_AVAILABLE:
        pytest.skip("Janome not installed, cannot test tokenization error scenario.")

    npt = prepare_instance
    # Ensure tokenizer is initialized for the test to reach the tokenize call
    if npt._janome_tokenizer is None:
        npt._janome_tokenizer = nlplot.nlplot.JanomeTokenizer() # Re-init if it was None due to prior test mocks

    text = "ã‚¨ãƒ©ãƒ¼ã‚’èµ·ã“ã™ãƒ†ã‚­ã‚¹ãƒˆ"
    tokens = npt._tokenize_japanese_text(text)
    assert tokens == []
    mock_print.assert_any_call(f"Error during Janome tokenization for text '{text[:30]}...': Simulated Janome Error")


# Test for get_japanese_text_features (replaces _initial version)
def test_get_japanese_text_features(prepare_instance):
    """
    (Green/Refactor Phase for TDD Cycle 2)
    Tests that get_japanese_text_features method calculates features correctly.
    """
    npt = prepare_instance
    if not nlplot.nlplot.JANOME_AVAILABLE:
        pytest.skip("Janome not installed, skipping japanese text features test.")
    if npt._janome_tokenizer is None:
         pytest.skip("Janome tokenizer not initialized in NLPlot instance, skipping japanese text features test.")

    sample_texts = pd.Series([
        "çŒ«ãŒçª“ã‹ã‚‰é¡”ã‚’å‡ºã—ãŸã€‚",             # text1
        "éå¸¸ã«ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã‚‹ã€‚",       # text2
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚",             # text3
        "ã€‚",                               # text4 (è¨˜å·ã®ã¿)
        "   ",                             # text5 (ç©ºç™½ã®ã¿)
        "",                                # text6 (ç©ºæ–‡å­—)
        None,                              # text7 (None)
        123,                               # text8 (éæ–‡å­—åˆ—, Janome treats "123" as åè©,æ•°)
        pd.NA                              # text9 (Pandas NA)
    ], dtype="object")

    features_df = npt.get_japanese_text_features(sample_texts)

    assert isinstance(features_df, pd.DataFrame)
    expected_columns = ['text', 'total_tokens', 'avg_token_length',
                        'noun_ratio', 'verb_ratio', 'adj_ratio', 'punctuation_count']
    assert list(features_df.columns) == expected_columns
    assert len(features_df) == len(sample_texts)

    # Expected values - calculated based on Janome's default tokenization
    # Note: avg_token_length definition: sum of lengths of non-punctuation tokens / count of non-punctuation tokens
    expected_values = [
        # text1: "çŒ«ãŒçª“ã‹ã‚‰é¡”ã‚’å‡ºã—ãŸã€‚" -> çŒ«,ãŒ,çª“,ã‹ã‚‰,é¡”,ã‚’,å‡ºã—,ãŸ,ã€‚ (9 tokens total)
        # Non-punct: çŒ«(1) ãŒ(1) çª“(1) ã‹ã‚‰(2) é¡”(1) ã‚’(1) å‡ºã—(2) ãŸ(1) (8 tokens, 10 chars)
        # Noun: çŒ«,çª“,é¡” (3). Verb(è‡ªç«‹): å‡ºã—(1). Adj(è‡ªç«‹): 0. Punct: ã€‚(1)
        {"text": "çŒ«ãŒçª“ã‹ã‚‰é¡”ã‚’å‡ºã—ãŸã€‚", 'total_tokens': 9, 'avg_token_length': 10/8 if 8>0 else 0.0,
         'noun_ratio': 3/9, 'verb_ratio': 1/9, 'adj_ratio': 0/9, 'punctuation_count': 1},
        # text2: "éå¸¸ã«ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã‚‹ã€‚" -> éå¸¸,ã«,ç¾ã—ã„,èŠ±,ãŒ,å’²ã„,ã¦,ã„ã‚‹,ã€‚ (9 tokens total)
        # Non-punct: éå¸¸(2),ã«(1),ç¾ã—ã„(3),èŠ±(1),ãŒ(1),å’²ã„(2),ã¦(1),ã„ã‚‹(2) (8 tokens, 13 chars)
        # Noun: éå¸¸,èŠ± (2). Verb(è‡ªç«‹): å’²ã„(1). Adj(è‡ªç«‹): ç¾ã—ã„(1). Punct: ã€‚(1) ('ã„ã‚‹' is å‹•è©,éè‡ªç«‹)
        {"text": "éå¸¸ã«ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã‚‹ã€‚", 'total_tokens': 9, 'avg_token_length': 13/8 if 8>0 else 0.0,
         'noun_ratio': 2/9, 'verb_ratio': 1/9, 'adj_ratio': 1/9, 'punctuation_count': 1},
        # text3: "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚" -> ä»Šæ—¥,ã¯,è‰¯ã„,å¤©æ°—,ã§ã™,ã­,ã€‚ (7 tokens total)
        # Non-punct: ä»Šæ—¥(2),ã¯(1),è‰¯ã„(2),å¤©æ°—(2),ã§ã™(2),ã­(1) (6 tokens, 10 chars)
        # Noun: ä»Šæ—¥,å¤©æ°— (2). Verb(è‡ªç«‹): 0. Adj(è‡ªç«‹): è‰¯ã„(1). Punct: ã€‚(1) ('ã§ã™' is åŠ©å‹•è©)
        {"text": "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚", 'total_tokens': 7, 'avg_token_length': 10/6 if 6>0 else 0.0,
         'noun_ratio': 2/7, 'verb_ratio': 0/7, 'adj_ratio': 1/7, 'punctuation_count': 1},
        # text4: "ã€‚" -> ã€‚ (1 token total)
        # Non-punct: 0 tokens, 0 chars
        # Noun:0, Verb:0, Adj:0, Punct: ã€‚(1)
        {"text": "ã€‚", 'total_tokens': 1, 'avg_token_length': 0.0,
         'noun_ratio': 0/1, 'verb_ratio': 0/1, 'adj_ratio': 0/1, 'punctuation_count': 1},
        # text5: "   " (ç©ºç™½ã®ã¿) -> _tokenize_japanese_text returns []
        {"text": "   ", 'total_tokens': 0, 'avg_token_length': 0.0,
         'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'punctuation_count': 0},
        # text6: "" (ç©ºæ–‡å­—) -> _tokenize_japanese_text returns []
        {"text": "", 'total_tokens': 0, 'avg_token_length': 0.0,
         'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'punctuation_count': 0},
        # text7: None -> original_text becomes "", _tokenize_japanese_text returns []
        {"text": "", 'total_tokens': 0, 'avg_token_length': 0.0,
         'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'punctuation_count': 0},
        # text8: 123 -> Janome: 123 (åè©,æ•°,*,*) (1 token)
        # Non-punct: 123 (1 token, 3 chars)
        {"text": "123", 'total_tokens': 1, 'avg_token_length': 3/1 if 1>0 else 0.0,
         'noun_ratio': 1/1, 'verb_ratio': 0/1, 'adj_ratio': 0/1, 'punctuation_count': 0},
        # text9: pd.NA -> original_text becomes "", _tokenize_japanese_text returns []
        {"text": "", 'total_tokens': 0, 'avg_token_length': 0.0,
         'noun_ratio': 0.0, 'verb_ratio': 0.0, 'adj_ratio': 0.0, 'punctuation_count': 0},
    ]

    for i, expected_row_dict in enumerate(expected_values):
        for col, expected_val in expected_row_dict.items():
            actual_val = features_df.iloc[i][col]
            if isinstance(expected_val, float):
                assert pytest.approx(actual_val, 0.01) == expected_val, f"Mismatch in {col} for text index {i} ('{sample_texts.iloc[i]}')"
            else:
                assert actual_val == expected_val, f"Mismatch in {col} for text index {i} ('{sample_texts.iloc[i]}')"


# --- Tests for stopwords combinations and min_edge_frequency ---
@pytest.mark.parametrize(
    "init_stopwords_file_content, method_stopwords, expected_combined_stopwords_check",
    [
        ("default1\ndefault2", [], lambda sw: "default1" in sw and "default2" in sw and "method1" not in sw and len(sw) == 2),
        (None, ["method1", "method2"], lambda sw: "method1" in sw and "method2" in sw and "default1" not in sw and len(sw) == 2),
        ("default1", ["method1"], lambda sw: "default1" in sw and "method1" in sw and len(sw) == 2),
        ("default1\ncommon", ["method1", "common"], lambda sw: "default1" in sw and "method1" in sw and "common" in sw and len(sw) == 3),
        (None, [], lambda sw: len(sw) == 0),
        ("", ["method1"], lambda sw: "method1" in sw and len(sw) == 1),
        ("default1", [], lambda sw: "default1" in sw and len(sw) == 1),
    ]
)
@patch("nlplot.nlplot.generate_freq_df")
def test_nlplot_stopwords_combination(
    mock_gen_freq_df, init_stopwords_file_content, method_stopwords, expected_combined_stopwords_check,
    prepare_data, tmp_path
):
    mock_gen_freq_df.return_value = pd.DataFrame(columns=["word", "word_count"])

    sw_file = None
    if init_stopwords_file_content is not None:
        p = tmp_path / "temp_stopwords.txt"
        p.write_text(init_stopwords_file_content)
        sw_file = str(p)

    npt = NLPlot(prepare_data.copy(), target_col="text", default_stopwords_file_path=sw_file)

    # Test a plotting method that uses generate_freq_df internally, like bar_ngram
    npt.bar_ngram(stopwords=method_stopwords, top_n=1)

    assert mock_gen_freq_df.call_count >= 1
    # Get the stopwords list that was actually passed to generate_freq_df
    # It's in called_kwargs if generate_freq_df was called with keyword arguments
    # Or in called_args if positional. Assuming keyword for 'stopwords'.
    final_stopwords_used = []
    if mock_gen_freq_df.call_args:
        args, kwargs = mock_gen_freq_df.call_args
        final_stopwords_used = kwargs.get("stopwords", [])

    assert expected_combined_stopwords_check(set(final_stopwords_used)) # Use set for easier comparison


def test_nlplot_min_edge_frequency_effect(prepare_data, tmp_path): # Using prepare_data instead of prepare_instance to have more control
    # Use a specific dataset for this test to have predictable co-occurrence counts
    texts = [
        "apple banana orange", # apple-banana, apple-orange, banana-orange (all 1)
        "apple banana kiwi",   # apple-banana (now 2), apple-kiwi (1), banana-kiwi (1)
        "apple banana grape",  # apple-banana (now 3), apple-grape (1), banana-grape (1)
        "orange kiwi grape"    # orange-kiwi (1), orange-grape (1), kiwi-grape (1)
    ]
    df = pd.DataFrame({'text': texts})
    output_dir = tmp_path / "mef_test_outputs"
    npt = NLPlot(df, target_col="text", output_file_path=str(output_dir))

    # Case 1: min_edge_frequency = 0 (all edges with freq > 0)
    npt.build_graph(stopwords=[], min_edge_frequency=0)
    edges_mef0 = len(npt.edge_df) if hasattr(npt, 'edge_df') and npt.edge_df is not None else 0
    # Expected edges: apple-banana (3), apple-orange (1), banana-orange (1), apple-kiwi (1), banana-kiwi (1)
    # apple-grape (1), banana-grape (1), orange-kiwi (1), orange-grape (1), kiwi-grape (1) -> Total 10 unique edges
    assert edges_mef0 == 10

    # Case 2: min_edge_frequency = 1 (edges with freq > 1, i.e., freq >= 2)
    npt.build_graph(stopwords=[], min_edge_frequency=1)
    edges_mef1 = len(npt.edge_df) if hasattr(npt, 'edge_df') and npt.edge_df is not None else 0
    # Only apple-banana (freq 3) should remain.
    assert edges_mef1 == 1
    if edges_mef1 == 1:
        edge = npt.edge_df.iloc[0]
        assert (edge['source'] == 'apple' and edge['target'] == 'banana') or \
               (edge['source'] == 'banana' and edge['target'] == 'apple')
        assert edge['edge_frequency'] == 3

    # Case 3: min_edge_frequency = 2 (edges with freq > 2, i.e., freq >= 3)
    npt.build_graph(stopwords=[], min_edge_frequency=2)
    edges_mef2 = len(npt.edge_df) if hasattr(npt, 'edge_df') and npt.edge_df is not None else 0
    # Only apple-banana (freq 3) should remain.
    assert edges_mef2 == 1 # Same as above

    # Case 4: min_edge_frequency = 3 (edges with freq > 3, i.e., freq >= 4)
    npt.build_graph(stopwords=[], min_edge_frequency=3)
    edges_mef3 = len(npt.edge_df) if hasattr(npt, 'edge_df') and npt.edge_df is not None else 0
    assert edges_mef3 == 0 # No edge with frequency > 3
    assert npt.node_df.empty if hasattr(npt, 'node_df') else True # No edges means no nodes in current impl.


# No teardown_module needed if all test outputs go to tmp_path
# and the global TTF_FONT_PATH is handled carefully or mocked appropriately where its absence is tested.
# The ensure_font_file_exists at the module level is for convenience for most tests.
# Tests for missing default font should mock nlplot.nlplot.DEFAULT_FONT_PATH.


def test_plot_japanese_text_features_initial(prepare_instance):
    """
    (Red Phase for TDD Cycle 3 - Optional Plotting Feature)
    Tests that plot_japanese_text_features method initially does not exist.
    """
    npt = prepare_instance
    dummy_features_data = {
        'text': ["text1", "text2"],
        'total_tokens': [10, 20],
        'avg_token_length': [1.5, 2.0],
        'noun_ratio': [0.3, 0.4],
        'verb_ratio': [0.2, 0.3],
        'adj_ratio': [0.1, 0.05],
        'punctuation_count': [1, 3]
    }
    dummy_features_df = pd.DataFrame(dummy_features_data)

    # This test is now replaced by the more comprehensive test_plot_japanese_text_features below
    pass

# Green/Refactor Phase for plotting (replaces _initial version)
def test_plot_japanese_text_features(prepare_instance):
    """
    (Green/Refactor Phase for TDD Cycle 3 - Optional Plotting Feature)
    Tests that plot_japanese_text_features method generates plots correctly.
    """
    npt = prepare_instance
    if not nlplot.nlplot.JANOME_AVAILABLE:
        pytest.skip("Janome not installed, skipping plot_japanese_text_features test as it relies on features_df.")
    if npt._janome_tokenizer is None:
         pytest.skip("Janome tokenizer not initialized, skipping plot_japanese_text_features test.")

    # Generate actual features_df using the existing method
    sample_texts = pd.Series([
        "çŒ«ãŒçª“ã‹ã‚‰é¡”ã‚’å‡ºã—ãŸã€‚",
        "éå¸¸ã«ç¾ã—ã„èŠ±ãŒå’²ã„ã¦ã„ã‚‹ã€‚",
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã¾ã—ã‚‡ã†ã‹ã€‚",
        "ç´ æ™´ã‚‰ã—ã„ä¸€æ—¥ã§ã—ãŸã€‚"
    ], dtype="object")
    features_df = npt.get_japanese_text_features(sample_texts)

    if features_df.empty or 'total_tokens' not in features_df.columns:
        pytest.skip("Feature DataFrame from get_japanese_text_features is unsuitable (empty or missing columns) for this plot test.")

    # --- Test for a valid feature ('total_tokens') ---
    target_col_to_plot = 'total_tokens'
    plot_title = "Distribution of Total Tokens"

    with patch("plotly.offline.plot") as mock_plotly_offline_plot:
        fig = npt.plot_japanese_text_features(
            features_df=features_df,
            target_feature=target_col_to_plot,
            title=plot_title,
            save=True,
            nbins=10
        )

        assert isinstance(fig, plotly.graph_objs.Figure), "Plot method did not return a Plotly Figure object."
        assert fig.layout.title.text == plot_title
        assert fig.layout.xaxis.title.text == target_col_to_plot
        assert len(fig.data) == 1

        mock_plotly_offline_plot.assert_called_once()
        call_args = mock_plotly_offline_plot.call_args
        assert call_args is not None
        saved_filename = call_args[1].get('filename', '')
        assert "jp_feature_total_tokens.html" in saved_filename

    # --- Test for another valid feature ('noun_ratio') ---
    target_col_ratio = 'noun_ratio'
    plot_title_ratio = "Distribution of Noun Ratio"
    fig_ratio = npt.plot_japanese_text_features(features_df=features_df, target_feature=target_col_ratio, title=plot_title_ratio)
    assert isinstance(fig_ratio, plotly.graph_objs.Figure)
    assert fig_ratio.layout.title.text == plot_title_ratio

    # --- Test for a non-existent feature ---
    with pytest.raises(ValueError, match="Target feature 'non_existent_feature' not found in DataFrame."):
        npt.plot_japanese_text_features(features_df=features_df, target_feature='non_existent_feature')

    # --- Test with empty DataFrame input ---
    empty_df_with_cols = pd.DataFrame(columns=features_df.columns)
    with pytest.raises(ValueError, match="Input DataFrame 'features_df' is empty or not a DataFrame."):
        npt.plot_japanese_text_features(features_df=empty_df_with_cols, target_feature='total_tokens')

    empty_df_no_cols = pd.DataFrame()
    with pytest.raises(ValueError, match="Input DataFrame 'features_df' is empty or not a DataFrame."):
        npt.plot_japanese_text_features(features_df=empty_df_no_cols, target_feature='total_tokens')

    # --- Test with target_feature column having all NaNs (after coercion) ---
    features_df_nan = features_df.copy()
    features_df_nan[target_col_to_plot] = np.nan
    with pytest.raises(ValueError, match=f"Column '{target_col_to_plot}' is not numeric, contains only NaN values, or could not be coerced to numeric for plotting."):
         npt.plot_japanese_text_features(features_df=features_df_nan, target_feature=target_col_to_plot)

    # --- Test with target_feature column being non-numeric and not coercible ---
    features_df_non_coercible = features_df.copy()
    features_df_non_coercible['string_col'] = ["abc", "def", "ghi", "jkl"] # Length matches sample_texts
    with pytest.raises(ValueError, match=f"Column 'string_col' is not numeric, contains only NaN values, or could not be coerced to numeric for plotting."):
        npt.plot_japanese_text_features(features_df=features_df_non_coercible, target_feature='string_col')
